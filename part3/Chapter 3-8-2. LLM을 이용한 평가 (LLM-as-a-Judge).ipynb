{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Colab Link URL**\n",
    "- https://colab.research.google.com/drive/1GjwYaiBaEGVcmAC1C5ItuyMgT2q6_HCG?usp=sharing\n",
    "\n",
    "이 노트북은 [Google Colab](https://colab.research.google.com/)에서 실행하시기에 최적화되어있습니다.\n",
    "상기 URL을 이용해서 코드를 실행 해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24263,
     "status": "ok",
     "timestamp": 1756080720587,
     "user": {
      "displayName": "Sungmin Han",
      "userId": "16133717747299446002"
     },
     "user_tz": -540
    },
    "id": "KwKq44a5jK-K",
    "outputId": "28a65352-5409-4946-e7b7-a26107e384b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q langchain-core langchain-community langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1756080766124,
     "user": {
      "displayName": "Sungmin Han",
      "userId": "16133717747299446002"
     },
     "user_tz": -540
    },
    "id": "DJij1CkI0urN"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from google.colab import userdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 625,
     "status": "ok",
     "timestamp": 1756080766751,
     "user": {
      "displayName": "Sungmin Han",
      "userId": "16133717747299446002"
     },
     "user_tz": -540
    },
    "id": "kPf8-8QOjrAI"
   },
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = userdata.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1756080766766,
     "user": {
      "displayName": "Sungmin Han",
      "userId": "16133717747299446002"
     },
     "user_tz": -540
    },
    "id": "JrZWAy5Y0v8H"
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-5-mini\", api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1756080766775,
     "user": {
      "displayName": "Sungmin Han",
      "userId": "16133717747299446002"
     },
     "user_tz": -540
    },
    "id": "8LbbT_73cep0"
   },
   "outputs": [],
   "source": [
    "eval_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"당신은 답변의 품질을 엄격하게 평가하는 AI 심판입니다.\n",
    "    질문: {question}\n",
    "    답변: {answer}\n",
    "\n",
    "\n",
    "    위 답변이 질문에 대해 얼마나 사실에 기반하여 충실하게 답변했는지 1점에서 5점 사이로 평가하고, 평가 이유를 JSON 형식으로 반환해주세요.\n",
    "    JSON 형식: {{\"score\": <점수>, \"reason\": \"<평가 이유>\"}}\n",
    "    \"\"\"\n",
    ")\n",
    "eval_chain = eval_prompt | llm | JsonOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8741,
     "status": "ok",
     "timestamp": 1756080766122,
     "user": {
      "displayName": "Sungmin Han",
      "userId": "16133717747299446002"
     },
     "user_tz": -540
    },
    "id": "xVCTUPl6dquQ",
    "outputId": "28396b78-9ef6-4873-9283-501da805c778"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 4, 'reason': '답변은 사실에 기반해 주요 장점(LLM의 할루시네이션 감소, 최신 정보 활용)을 정확히 지적하고 있음. 다만 과장된 단정(완전히 할루시네이션을 없앤다/항상 최신 정보를 사용한다)은 피해야 하며, RAG가 검색 품질·문서 신뢰도에 의존하고 여전히 환각이 발생할 수 있다는 한계나, 검색 기반의 사실성 향상·도메인 적응성·확장성 등 다른 장점들이 언급되지 않아 설명이 다소 불충분함.'}\n"
     ]
    }
   ],
   "source": [
    "# 평가 실행\n",
    "evaluation_result = eval_chain.invoke({\n",
    "    \"question\": \"RAG의 장점은 무엇인가?\",\n",
    "    \"answer\": \"RAG는 LLM의 할루시네이션을 줄여주고, 답변을 최신 정보에 기반하게 합니다.\"\n",
    "})\n",
    "print(evaluation_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n2Wc2J6Le-fU"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOLcUeV+VL5mGlsqZ8xfRTM",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
