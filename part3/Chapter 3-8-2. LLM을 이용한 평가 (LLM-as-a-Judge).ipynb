{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOLcUeV+VL5mGlsqZ8xfRTM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install -q langchain-core langchain-community langchain-openai"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KwKq44a5jK-K","executionInfo":{"status":"ok","timestamp":1756080720587,"user_tz":-540,"elapsed":24263,"user":{"displayName":"Sungmin Han","userId":"16133717747299446002"}},"outputId":"28a65352-5409-4946-e7b7-a26107e384b7"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["from langchain_openai import ChatOpenAI\n","from langchain_core.prompts import ChatPromptTemplate\n","from langchain_core.output_parsers import JsonOutputParser\n","from google.colab import userdata"],"metadata":{"id":"DJij1CkI0urN","executionInfo":{"status":"ok","timestamp":1756080766124,"user_tz":-540,"elapsed":4,"user":{"displayName":"Sungmin Han","userId":"16133717747299446002"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["OPENAI_API_KEY = userdata.get(\"OPENAI_API_KEY\")"],"metadata":{"id":"kPf8-8QOjrAI","executionInfo":{"status":"ok","timestamp":1756080766751,"user_tz":-540,"elapsed":625,"user":{"displayName":"Sungmin Han","userId":"16133717747299446002"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["llm = ChatOpenAI(model=\"gpt-5-mini\", api_key=OPENAI_API_KEY)"],"metadata":{"id":"JrZWAy5Y0v8H","executionInfo":{"status":"ok","timestamp":1756080766766,"user_tz":-540,"elapsed":11,"user":{"displayName":"Sungmin Han","userId":"16133717747299446002"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["eval_prompt = ChatPromptTemplate.from_template(\n","    \"\"\"당신은 답변의 품질을 엄격하게 평가하는 AI 심판입니다.\n","    질문: {question}\n","    답변: {answer}\n","\n","\n","    위 답변이 질문에 대해 얼마나 사실에 기반하여 충실하게 답변했는지 1점에서 5점 사이로 평가하고, 평가 이유를 JSON 형식으로 반환해주세요.\n","    JSON 형식: {{\"score\": <점수>, \"reason\": \"<평가 이유>\"}}\n","    \"\"\"\n",")\n","eval_chain = eval_prompt | llm | JsonOutputParser()"],"metadata":{"id":"8LbbT_73cep0","executionInfo":{"status":"ok","timestamp":1756080766775,"user_tz":-540,"elapsed":7,"user":{"displayName":"Sungmin Han","userId":"16133717747299446002"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["# 평가 실행\n","evaluation_result = eval_chain.invoke({\n","    \"question\": \"RAG의 장점은 무엇인가?\",\n","    \"answer\": \"RAG는 LLM의 할루시네이션을 줄여주고, 답변을 최신 정보에 기반하게 합니다.\"\n","})\n","print(evaluation_result)"],"metadata":{"id":"xVCTUPl6dquQ","executionInfo":{"status":"ok","timestamp":1756080766122,"user_tz":-540,"elapsed":8741,"user":{"displayName":"Sungmin Han","userId":"16133717747299446002"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"28396b78-9ef6-4873-9283-501da805c778"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["{'score': 4, 'reason': '답변은 사실에 기반해 주요 장점(LLM의 할루시네이션 감소, 최신 정보 활용)을 정확히 지적하고 있음. 다만 과장된 단정(완전히 할루시네이션을 없앤다/항상 최신 정보를 사용한다)은 피해야 하며, RAG가 검색 품질·문서 신뢰도에 의존하고 여전히 환각이 발생할 수 있다는 한계나, 검색 기반의 사실성 향상·도메인 적응성·확장성 등 다른 장점들이 언급되지 않아 설명이 다소 불충분함.'}\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"n2Wc2J6Le-fU"},"execution_count":null,"outputs":[]}]}