{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMPiD6fCBbgj0AwWXMXEtFz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"h8oLa_dMSJHL","executionInfo":{"status":"ok","timestamp":1756078650021,"user_tz":-540,"elapsed":22823,"user":{"displayName":"Sungmin Han","userId":"16133717747299446002"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"4165daaf-6e84-48b8-e303-c1b7da6d8148"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -q langchain-core langchain-community langchain-openai faiss-cpu"]},{"cell_type":"code","source":["from langchain_openai import ChatOpenAI\n","from google.colab import userdata"],"metadata":{"id":"AmzaPqh50yP0","executionInfo":{"status":"ok","timestamp":1756078656621,"user_tz":-540,"elapsed":6602,"user":{"displayName":"Sungmin Han","userId":"16133717747299446002"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["OPENAI_API_KEY = userdata.get(\"OPENAI_API_KEY\")"],"metadata":{"id":"O_VuPA-f0zff","executionInfo":{"status":"ok","timestamp":1756078657164,"user_tz":-540,"elapsed":540,"user":{"displayName":"Sungmin Han","userId":"16133717747299446002"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# OpenAI의 gpt-5-mini 모델을 사용하는 모델 객체 생성\n","llm = ChatOpenAI(\n","    openai_api_key=OPENAI_API_KEY,\n","    model=\"gpt-5-mini\",\n","    temperature=0,\n",")\n","\n","# 표준화된 .invoke() 메서드로 모델 호출\n","response = llm.invoke(\"대한민국의 수도는 어디인가요?\")\n","print(response.content)"],"metadata":{"id":"ZnExs3saU9xP","executionInfo":{"status":"ok","timestamp":1756078662551,"user_tz":-540,"elapsed":5385,"user":{"displayName":"Sungmin Han","userId":"16133717747299446002"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"43f11b93-7e3a-4ab5-f3cc-ce5dce00d7d0"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["대한민국의 수도는 서울특별시(서울)입니다. 다만 행정 기능 일부는 세종특별자치시로 이전되어 정부 부처 일부가 세종에 있습니다.\n"]}]},{"cell_type":"code","source":["# Prompts 구성 요소 사용 예시\n","from langchain_core.prompts import ChatPromptTemplate\n","\n","# {topic}과 {language}라는 두 개의 입력 변수를 가진 프롬프트 템플릿 생성\n","prompt_template = ChatPromptTemplate.from_template(\n","    \"{topic}에 대해 {language}로 간략히 설명해주세요.\"\n",")\n","\n","# 템플릿에 변수 값을 채워 실제 프롬프트를 생성\n","prompt_filled = prompt_template.format_prompt(topic=\"인공지능\", language=\"한국어\")\n","print(prompt_filled.to_messages())"],"metadata":{"id":"7yHFmf8wU_cR","executionInfo":{"status":"ok","timestamp":1756078662723,"user_tz":-540,"elapsed":168,"user":{"displayName":"Sungmin Han","userId":"16133717747299446002"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"68d09c3e-4613-4605-c291-0d9d7ab5ef03"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[HumanMessage(content='인공지능에 대해 한국어로 간략히 설명해주세요.', additional_kwargs={}, response_metadata={})]\n"]}]},{"cell_type":"code","source":["# Output Parsers 구성 요소 사용 예시\n","from langchain_core.output_parsers import JsonOutputParser\n","\n","# JSON 형식으로 답변을 생성하도록 유도하는 프롬프트\n","json_prompt = \"다음 인물에 대한 정보를 JSON 형식으로 제공해줘: Elon Musk\"\n","# LLM이 생성한 JSON 문자열을 파이썬 딕셔너리로 변환하는 파서\n","json_parser = JsonOutputParser()\n","\n","# 가상의 LLM 출력 (실제로는 llm.invoke의 결과)\n","llm_output_string = '{\"name\": \"Elon Musk\", \"company\": [\"SpaceX\", \"Tesla\"], \"known_for\": [\"electric cars\", \"reusable rockets\"]}'\n","\n","# 파서를 사용하여 문자열을 딕셔너리로 변환\n","parsed_output = json_parser.parse(llm_output_string)\n","\n","print(type(parsed_output))\n","print(parsed_output['company'])"],"metadata":{"id":"zIU8BiWhVMOa","executionInfo":{"status":"ok","timestamp":1756078662788,"user_tz":-540,"elapsed":61,"user":{"displayName":"Sungmin Han","userId":"16133717747299446002"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f3b79181-f8b1-4159-d378-d55ee9d31af0"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'dict'>\n","['SpaceX', 'Tesla']\n"]}]},{"cell_type":"code","source":["# LCEL 사용 예시\n","from langchain_core.output_parsers.string import StrOutputParser\n","\n","# 앞서 정의한 구성 요소들을 파이프로 연결한다.\n","chain = prompt_template | llm | StrOutputParser()\n","\n","# 체인 실행\n","result = chain.invoke({\"topic\": \"대규모 언어 모델\", \"language\": \"영어\"})\n","print(result)"],"metadata":{"id":"Z4yhHYcH236E","executionInfo":{"status":"ok","timestamp":1756078670326,"user_tz":-540,"elapsed":7540,"user":{"displayName":"Sungmin Han","userId":"16133717747299446002"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"40b8702a-63c3-463f-91e2-aff4dfc1a848"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["A large language model (LLM) is a type of AI trained on vast amounts of text to predict and generate human-like language. Key points:\n","\n","- Architecture and training: Most modern LLMs use the transformer architecture and are pretrained on huge text corpora to learn patterns of grammar, facts, and style. They are often later fine-tuned for specific tasks.\n","- How they work: Given some input text, they predict the most likely next words (autoregressive models) or fill in masked words (masked-language models), enabling generation, completion, and understanding of language.\n","- Capabilities: They can write and summarize text, translate languages, answer questions, generate code, and help with brainstorming or tutoring.\n","- Limitations and risks: They can produce incorrect or misleading information (“hallucinations”), reflect biases present in training data, and may leak sensitive information. They do not possess true understanding or consciousness.\n","- Practical use and safety: Effective use often combines human oversight, prompt engineering, fine-tuning, and safety controls (content filters, auditing) to reduce errors and harms.\n","\n","Examples of LLMs include GPT-series (GPT-3/4), BERT, PaLM, and LLaMA.\n"]}]},{"cell_type":"code","source":["# Retrieval 구성 요소 사용 예시\n","from langchain_openai import OpenAIEmbeddings\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain_community.document_loaders import WebBaseLoader\n","from langchain_community.vectorstores import FAISS\n","from google.colab import userdata\n","\n","# 1. Load: 웹 페이지 로더로 '대형 언어 모델' 위키피디아 문서 로드\n","loader = WebBaseLoader(\"https://ko.wikipedia.org/wiki/%EB%8C%80%ED%98%95_%EC%96%B8%EC%96%B4_%EB%AA%A8%EB%8D%B8\")\n","docs = loader.load()\n","\n","# 2. Split: 텍스트 분할기로 문서를 500자 단위로 분할\n","text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n","splits = text_splitter.split_documents(docs)\n","\n","# 3. Embed & Store: OpenAI 임베딩 모델과 FAISS 벡터 저장소 생성\n","# 분할된 문서들은 벡터로 변환되어 FAISS에 저장된다.\n","vectorstore = FAISS.from_documents(documents=splits, embedding=OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY))\n","\n","# 4. Retrieve: 검색기(Retriever)를 생성하고 관련 문서 검색\n","retriever = vectorstore.as_retriever()\n","retrieved_docs = retriever.invoke(\"LLM의 문제점은 무엇인가?\")\n","\n","# 검색된 문서의 내용 중 첫 번째 문서의 일부를 출력\n","print(retrieved_docs[0].page_content[:200]) # 출력 내용이 길어 일부만 표시"],"metadata":{"id":"VyxbqztA3-Tj","executionInfo":{"status":"ok","timestamp":1756078674776,"user_tz":-540,"elapsed":4437,"user":{"displayName":"Sungmin Han","userId":"16133717747299446002"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a26bddcf-15f1-4860-b6b2-e561274853c6"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"]},{"output_type":"stream","name":"stdout","text":["로봇공학, 소프트웨어 공학, 사회적 영향 연구 등의 컴퓨터 과학의 여러 하위 분야에서 LLM의 사용 증가로 이어졌다.[16] 2024년에는 오픈AI가 논리적 추론 기능을 강화한 모델 OpenAI o1을 출시했으며, 이 모델은 최종 답변을 생성하기 전에 장문의 사고 과정(Chains of Thought)을 먼저 생성하는 특징이 있다.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"iGORks-hWUmk","executionInfo":{"status":"ok","timestamp":1756078674791,"user_tz":-540,"elapsed":9,"user":{"displayName":"Sungmin Han","userId":"16133717747299446002"}}},"execution_count":8,"outputs":[]}]}